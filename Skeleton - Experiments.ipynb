{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy as sci\n",
    "import scipy.stats as ss\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "import sys, math, os, copy, time, csv, itertools\n",
    "import graphviz as gv\n",
    "import covar\n",
    "import networkx as nx\n",
    "import pyBN as pybn\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from scipy.stats import norm, beta\n",
    "from scipy.special import gamma\n",
    "from itertools import combinations, permutations\n",
    "from math import log, exp, log2\n",
    "from operator import itemgetter, attrgetter\n",
    "from inverse_covariance import ModelAverage, QuicGraphLassoEBIC, QuicGraphLasso, QuicGraphLassoCV,AdaptiveGraphLasso\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov2cor( A ):\n",
    "    \"\"\"\n",
    "    covariance matrix to correlation matrix.\n",
    "    \"\"\"\n",
    "    d = np.sqrt(A.diagonal())\n",
    "    A = ((A.T/d).T)/d\n",
    "    return A\n",
    "\n",
    "def cov2pcor(cov):\n",
    "#    cov = np.cov(X, rowvar=False)\n",
    "    D, _ = cov.shape\n",
    "    sigma = -np.linalg.pinv(cov)\n",
    "    for i in range(D):\n",
    "        sigma[i,i] = -1*sigma[i,i]\n",
    "    pi_obs = cov2cor(sigma)\n",
    "    return pi_obs\n",
    "\n",
    "def pcor2cov(pi):\n",
    "    pi = -pi\n",
    "    for i in range(D):\n",
    "        pi[i,i] = -1*pi[i,i]\n",
    "    \n",
    "    return np.linalg.inv(pi)\n",
    "\n",
    "def pcor2prec(pi):\n",
    "    pi = -pi\n",
    "    for i in range(D):\n",
    "        pi[i,i] = -1*pi[i,i]\n",
    "    return pi\n",
    "\n",
    "def prec2pcor(pi):\n",
    "    pi = cov2cor(pi)\n",
    "    return pi\n",
    "\n",
    "def generate_graph(pi):\n",
    "    D, _ = pi.shape\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(D))   \n",
    "    \n",
    "    for i in range(D):\n",
    "        for j in range(i):\n",
    "            if abs(pi[i, j]) > 1e-8:\n",
    "                G.add_edge(i, j)\n",
    "            else:\n",
    "                pass\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "def generate_empty_graph(pi):\n",
    "    D, _ = pi.shape\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(D))   \n",
    "    return G\n",
    "\n",
    "def calculate_entropy(X, steps):\n",
    "    # establish ranges for X\n",
    "    lo, hi, N = min(X), max(X) + 1e-8, X.shape[0]\n",
    "    entropy = 0.0\n",
    "    for i in range(steps):\n",
    "        xl = (lo + (hi-lo)*(i*1.0/steps))\n",
    "        xh = lo + (hi - lo)*( (i+1.0) / steps)\n",
    "        tmp = ((xl <= X) & (X < xh)).sum()\n",
    "        if tmp == 0:\n",
    "            continue\n",
    "        else:\n",
    "            p = 1.0 * tmp / N\n",
    "            entropy -= p * log2(p)\n",
    "    return entropy\n",
    "    \n",
    "def calculate_joint_entropy(X, Y, steps):\n",
    "    X_lo, X_hi, N = min(X), max(X) + 1e-8, X.shape[0]\n",
    "    Y_lo, Y_hi = min(Y), max(Y) + 1e-8\n",
    "    entropy = 0.0\n",
    "    cnt = 0\n",
    "    for i in range(steps):\n",
    "        for j in range(steps):\n",
    "            xl = X_lo + (X_hi - X_lo)*(i*1.0/steps)\n",
    "            xh = X_lo + (X_hi - X_lo)*( (i+1.0) / steps)\n",
    "            \n",
    "            yl = Y_lo + (Y_hi - Y_lo)*(j*1.0/steps)\n",
    "            yh = Y_lo + (Y_hi - Y_lo)*( (j+1.0) / steps)\n",
    "        \n",
    "            tmp = ((xl <= X) & (X < xh) & (yl <= Y) & (Y < yh)).sum()\n",
    "            if tmp == 0:\n",
    "                continue\n",
    "            else:\n",
    "                cnt += tmp\n",
    "                p = 1.0 * tmp / N\n",
    "                entropy -= p * log2(p)\n",
    "    return entropy\n",
    "    \n",
    "def calculate_mutual_information(X, Y, steps=10):\n",
    "    h_X = calculate_entropy(X, steps)\n",
    "    h_Y = calculate_entropy(Y, steps)\n",
    "    h_XY = calculate_joint_entropy(X, Y, steps)\n",
    "    return h_X + h_Y - h_XY\n",
    "\n",
    "def partial_corr(X, i, j, k):\n",
    "    D = X.shape[1]\n",
    "    idx = np.zeros(D, dtype=np.bool)\n",
    "    for k_val in k:\n",
    "        idx[k_val] = True\n",
    "    \n",
    "    if len(k) > 0:\n",
    "        beta_i = sci.linalg.lstsq(X[:, idx], X[:, j])[0]\n",
    "        beta_j = sci.linalg.lstsq(X[:, idx], X[:, i])[0]\n",
    "\n",
    "        res_j = X[:, j] - X[:, idx].dot(beta_i)\n",
    "        res_i = X[:, i] - X[:, idx].dot(beta_j)\n",
    "\n",
    "        corr = ss.pearsonr(res_i, res_j)[0]\n",
    "    else:\n",
    "        corr = ss.pearsonr(X[:, i], X[:, j])[0]\n",
    "    \n",
    "    return corr\n",
    "            \n",
    "def fisher_test(X, i, j, k):\n",
    "    N, _ = X.shape\n",
    "    pcorr = partial_corr(X, i, j, k)\n",
    "    if 1.0 - pcorr**2 < 1e-9:\n",
    "        pval = 1e-10\n",
    "    else:\n",
    "        tij = pcorr * (N - len(k) - 2)**0.5 / ((1.0 - (pcorr**2))**0.5)\n",
    "        pval = 2*ss.t.sf(abs(tij), N - len(k) - 2)\n",
    "    return pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAUSSIAN DATA SIMULATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_correlation_matrix(num_nodes, frac_edges, mean=0):\n",
    "    N = num_nodes\n",
    "    mu_A = frac_edges\n",
    "    eps = 10**-4\n",
    "    cors = np.random.uniform(low=-1.0, high=1.0, size=(N, N))\n",
    "    edges = np.random.choice(2, size=(N,N), p=(1.0 - frac_edges, frac_edges))\n",
    "    edges = np.tril(edges, -1)\n",
    "    edges = np.multiply(edges, cors)\n",
    "    edges = edges + np.transpose(edges)\n",
    "    abs_edges = np.absolute(edges)\n",
    "    \n",
    "    col_sums = np.sum(abs_edges, axis=0) # columnwise\n",
    "    col_sums = np.maximum(col_sums, np.ones(D))\n",
    "    col_sums = np.add(col_sums, np.array([eps]*N))\n",
    "    col_sums = np.diag(col_sums)\n",
    "    \n",
    "    pi = col_sums + edges\n",
    "    pi = cov2cor(pi)\n",
    "    return pi\n",
    "\n",
    "# Returns samples of data multivariate Gaussian distributed\n",
    "\n",
    "def simulate_gaussian_data(num_samples, dim, frac_edges, pcor=None):\n",
    "    N = num_samples\n",
    "    D = dim\n",
    "    \n",
    "    if pcor is None:\n",
    "        pcor = simulate_correlation_matrix(D, frac_edges)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    P = pcor2cov(pcor)\n",
    "    sample_data = np.random.multivariate_normal(mean=np.array([0.0]*D),\n",
    "                                                cov=P,\n",
    "                                                size=N)\n",
    "    \n",
    "    return sample_data, pcor\n",
    "\n",
    "def nx2graphviz(nxg, node_names=None):\n",
    "    G = gv.Graph('G')\n",
    "    for _, v in enumerate(list(nxg.nodes())):\n",
    "        if node_names is None:\n",
    "            G.node(str(v), str(v))\n",
    "        else:\n",
    "            G.node(node_names[v], node_names[v])\n",
    "            \n",
    "    for _, v in enumerate(list(nxg.edges())):\n",
    "        i, j = v\n",
    "        if node_names is None:\n",
    "            i, j = str(i), str(j)\n",
    "        else:\n",
    "            i, j = node_names[i], node_names[j]\n",
    "        G.edge(i, j)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NETWORK METRIC CALCULATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network_metrics(g_true, g_pred, time_diff=0.0):\n",
    "    D = len(g_true.nodes())\n",
    "    actual, pred = np.zeros((D, D)), np.zeros((D, D))\n",
    "    for e in g_true.edges():\n",
    "        i, j = e\n",
    "        actual[i,j], actual[j,i] = 1, 1\n",
    "    for e in g_pred.edges():\n",
    "        i, j = e\n",
    "        pred[i,j], pred[j,i] = 1, 1\n",
    "    il = np.tril_indices(D, k=-1)\n",
    "    actual, pred = actual[il], pred[il]\n",
    "    \n",
    "    cf = confusion_matrix(actual, pred)\n",
    "    \n",
    "    ### Metrics to be returned\n",
    "    tn, fp, fn, tp = cf[0][0], cf[0][1], cf[1][0], cf[1][1]\n",
    "    \n",
    "    sens = tp / (tp + fn)\n",
    "    spec = tn / (tn + fp)\n",
    "    fpr = fp / (fp + tn)\n",
    "    ppv = tp / (tp + fp)\n",
    "    denominator = (tp+fp) * (tp + fn) * (tn + fp) * (tn+fn)\n",
    "    if denominator < 1e-8:\n",
    "        denominator = 1.0\n",
    "    mcc = (tp * tn - fp * fn) / ( denominator )**0.5\n",
    "    \n",
    "    if sens < 1e-9 or ppv < 1e-9:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2.0 / ( (1.0/sens) + (1.0/ppv))\n",
    "    \n",
    "    hd = sum(abs(pred*1 - actual*1)) \n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "    \n",
    "    return (tp, fp, tn, fn, sens, spec, fpr, ppv, mcc, f1_score, accuracy, hd, time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RELEVANCE NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevanceNetworks():\n",
    "    \n",
    "    def __init__(self, threshold=0.99):\n",
    "        self._alpha = 0.99\n",
    "        self.scores_ = None\n",
    "        self.network_ = None\n",
    "    \n",
    "    def fit(self, sample_data):\n",
    "        X = sample_data\n",
    "        N, D = X.shape\n",
    "        mutual_inf = np.zeros((D,D))\n",
    "        \n",
    "        for i in range(D):\n",
    "            for j in range(i):\n",
    "                mi = calculate_mutual_information(X[:, i], X[:, j], steps=int(log2(N)))\n",
    "                mutual_inf[i, j], mutual_inf[j, i] = mi, mi\n",
    "\n",
    "        # Calculate the MI threshold using permutations\n",
    "        permutated_mi_vals = []\n",
    "        for tt in range(1000):\n",
    "            idx1 = np.random.randint(0, D)\n",
    "            idx2 = np.random.randint(0, D)\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "            A, B = X[:, idx1], X[:, idx2]\n",
    "            A = np.random.permutation(A)\n",
    "            mi = calculate_mutual_information(A, B, steps=int(log2(N)))\n",
    "            permutated_mi_vals.append(mi)\n",
    "\n",
    "        permutated_mi_vals = sorted(permutated_mi_vals)\n",
    "        threshold = permutated_mi_vals[int(len(permutated_mi_vals) * self._alpha)]\n",
    "        \n",
    "        gr_pred = nx.Graph()\n",
    "        gr_pred.add_nodes_from(range(D))\n",
    "        for i in range(D):\n",
    "            for j in range(i):\n",
    "                if mutual_inf[i, j] > threshold:\n",
    "                    gr_pred.add_edge(i,j)\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        self.scores_ = mutual_inf\n",
    "        self.network_ = gr_pred\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FISHER APPROXIMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FisherApproximation():\n",
    "    \n",
    "    def __init__(self, threshold=0.95, bonf_correction=True):\n",
    "        self._alpha = threshold\n",
    "        self._use_bonf = bonf_correction\n",
    "        self.scores_ = None\n",
    "        self.network_ = None\n",
    "    \n",
    "    def fit(self, sample_data):\n",
    "        X = sample_data\n",
    "        N, D = X.shape    \n",
    "        scores = np.zeros((D,D))\n",
    "        cov = np.cov(X, rowvar=False)\n",
    "\n",
    "        sigma = -np.linalg.inv(cov)\n",
    "        for i in range(D):\n",
    "            sigma[i,i] = -1*sigma[i,i]\n",
    "        pi_obs = cov2cor(sigma)\n",
    "\n",
    "        if self._use_bonf:\n",
    "            n_tests = 0.5*D*(D-1)\n",
    "            threshold = self._alpha / n_tests\n",
    "        else:\n",
    "            threshold = self._alpha\n",
    "        \n",
    "        gr_pred = nx.Graph()\n",
    "        gr_pred.add_nodes_from(range(D))\n",
    "        \n",
    "        for i in range(D):\n",
    "            for j in range(i):\n",
    "                v = pi_obs[i,j]\n",
    "                # Do the fisher transform\n",
    "                zij = 0.5*log( (1+v) / (1-v) )\n",
    "                zscore = (N - D + 2 - 3)**0.5 * abs(zij)\n",
    "                scores[i,j] = 1 - 2*(1 - norm.cdf(zscore))\n",
    "                scores[j,i] = scores[i,j]\n",
    "                \n",
    "                if norm.cdf(zscore) > 1 - 0.5*threshold:\n",
    "                    gr_pred.add_edge(i,j)\n",
    "                else:\n",
    "                    pass\n",
    "        self.scores_ = scores\n",
    "        self.network_ = gr_pred\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAlgorithm():\n",
    "    \n",
    "    def __init__(self, threshold=0.99, stable=True, init_graph=None):\n",
    "        self._alpha = 1.0 - threshold\n",
    "        self._stable = stable\n",
    "        self.scores_ = None\n",
    "        self.network_ = None\n",
    "        self._init_graph = init_graph\n",
    "        \n",
    "    def fit(self, sample_data):\n",
    "        X = sample_data\n",
    "        N, D = X.shape\n",
    "        node_ids = range(X.shape[1])\n",
    "\n",
    "        if self._init_graph is None:\n",
    "            g = nx.Graph()\n",
    "            g.add_nodes_from(node_ids)\n",
    "            for (i, j) in combinations(node_ids, 2):\n",
    "                g.add_edge(i, j)\n",
    "        else:\n",
    "            g = self._init_graph\n",
    "        \n",
    "        l = 0\n",
    "        max_reach = min(15, D-2)\n",
    "        while True:\n",
    "            cont = False\n",
    "            remove_edges = []\n",
    "            for (i, j) in permutations(node_ids, 2):\n",
    "                adj_i = list(g.neighbors(i))\n",
    "                if j not in adj_i:\n",
    "                    continue\n",
    "                else:\n",
    "                    adj_i.remove(j)\n",
    "                    pass\n",
    "                if len(adj_i) >= l:\n",
    "                    if len(adj_i) < l:\n",
    "                        continue\n",
    "                    for k in combinations(adj_i, l):\n",
    "                        p_val = fisher_test(X, i, j, set(k))\n",
    "                        if p_val > self._alpha:\n",
    "                            if g.has_edge(i, j):\n",
    "                                if self._stable:\n",
    "                                    remove_edges.append((i, j))\n",
    "                                else:\n",
    "                                    g.remove_edge(i, j)\n",
    "                                pass\n",
    "                            break\n",
    "                        pass\n",
    "                    cont = True\n",
    "                    pass\n",
    "                pass\n",
    "            l += 1\n",
    "            if self._stable:\n",
    "                g.remove_edges_from(remove_edges)\n",
    "            if cont is False:\n",
    "                break\n",
    "            if l > max_reach:\n",
    "                break\n",
    "            pass\n",
    "\n",
    "        self.network_ = g\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InterIAMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapated from the pyBN repository \n",
    "def resolve_markov_blanket(Mb, data, alpha):\n",
    "    n_rv = data.shape[1]\n",
    "    \n",
    "    Bd = copy.deepcopy(Mb)\n",
    "    for X in range(n_rv):\n",
    "        for Y in Mb[X]:\n",
    "            if X not in Mb[Y]:\n",
    "                Bd[X].remove(Y)\n",
    "                #Bd[Y].append(X)\n",
    "    \n",
    "    for X in range(n_rv):\n",
    "        for Y in Mb[X]:\n",
    "            if (Y not in Bd[X] and X in Bd[Y]) or (X not in Bd[Y] and Y in Bd[X]):\n",
    "                print(\"Neighbourhood boundary error!\")\n",
    "            elif (X not in Bd[Y]):\n",
    "                # Edge already removed from graph\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if len(set(Bd[X]) - set([Y])) < len(set(Bd[Y]) - set([X])):\n",
    "                T = list(set(Bd[X]) - set([Y]))\n",
    "            else:\n",
    "                T = list(set(Bd[Y]) - set([X]))\n",
    "                \n",
    "            for i in range(len(T)):\n",
    "                removed_edge = False\n",
    "                for S in itertools.combinations(T,i):\n",
    "                    pval = fisher_test(data, X, Y, set(S))\n",
    "                    if pval > alpha:\n",
    "                        #print(X, Y, Bd[X], Mb[X], Bd[Y], Mb[Y])\n",
    "                        Bd[X].remove(Y)\n",
    "                        Bd[Y].remove(X)\n",
    "                        removed_edge=True\n",
    "                        break\n",
    "                    else:\n",
    "                        pass\n",
    "                if removed_edge:\n",
    "                    break\n",
    "\n",
    "    edge_dict = dict([(rv,[]) for rv in range(n_rv)])\n",
    "    for X in range(n_rv):\n",
    "        for Y in Bd[X]:\n",
    "            edge_dict[Y].append(X)\n",
    "    \n",
    "    return edge_dict\n",
    "\n",
    "class InterIAMBAlgorithm():\n",
    "    \n",
    "    def __init__(self, threshold=0.99):\n",
    "        self._alpha = 1.0 - threshold\n",
    "        self.scores_ = None\n",
    "        self.network_ = None\n",
    "\n",
    "    def fit(self, sample_data):\n",
    "        N, D = sample_data.shape\n",
    "        node_ids = range(sample_data.shape[1])\n",
    "\n",
    "        g = nx.Graph()\n",
    "        g.add_nodes_from(node_ids)\n",
    "        \n",
    "        # Learn the markov blankets\n",
    "        mb = dict([(node,[]) for node in node_ids])\n",
    "        node_set = set(node_ids)\n",
    "        \n",
    "        for T in node_ids:\n",
    "            # Phase 1 - Forward Phase\n",
    "            CMB = set([])\n",
    "            change = True\n",
    "            while change:\n",
    "                best = [1e8, -1]\n",
    "                for X in node_ids:\n",
    "                    if X != T and X not in CMB:\n",
    "                        pv = fisher_test(sample_data, T, X, CMB)\n",
    "                        if pv < best[0]:\n",
    "                            best = [pv, X]\n",
    "                pval, X = best\n",
    "                if pval < self._alpha:\n",
    "                    CMB.add(X)\n",
    "                    # Phase 2 - Shrink Phase\n",
    "                    to_remove = set([])\n",
    "                    for Y in list(CMB):\n",
    "                        pv = fisher_test(sample_data, T, Y, CMB - set([Y]))\n",
    "                        if pv >= self._alpha:\n",
    "                            CMB.remove(Y)\n",
    "                else:\n",
    "                    change = False\n",
    "            \n",
    "            mb[T] = list(CMB)\n",
    "\n",
    "        # Resolve the markov blankets\n",
    "        edge_dict = resolve_markov_blanket(mb, sample_data, alpha=self._alpha)\n",
    "        for node in edge_dict:\n",
    "            edges = edge_dict[node]\n",
    "            for child in edges:\n",
    "                g.add_edge(node, child)\n",
    "        \n",
    "        self.network_ = g\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicalLasso():\n",
    "    def __init__(self, threshold=0.0, use_ebic=False, use_adaptive=False, gamma=0.0):\n",
    "        self._threshold = threshold\n",
    "        self._use_ebic = use_ebic\n",
    "        self._use_adaptive = use_adaptive\n",
    "        self._gamma = gamma\n",
    "        \n",
    "        self.scores_ = None\n",
    "        self.network_ = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        N, D = X.shape\n",
    "        g = nx.Graph()\n",
    "        g.add_nodes_from(range(D))\n",
    "        scores = np.zeros((D,D))\n",
    "        \n",
    "        if self._use_ebic:\n",
    "            estimator = QuicGraphLassoEBIC(gamma=self._gamma, init_method='cov')\n",
    "        else:\n",
    "            estimator = QuicGraphLassoCV(init_method='cov')\n",
    "            \n",
    "        if self._use_adaptive:\n",
    "            model = AdaptiveGraphLasso(estimator=estimator,  method='inverse')\n",
    "            model.fit(X)\n",
    "            pi = prec2pcor(model.estimator_.precision_)\n",
    "        else:\n",
    "            model = estimator\n",
    "            model.fit(X)\n",
    "            pi = prec2pcor(model.precision_)\n",
    "       \n",
    "        for i in range(D):\n",
    "            for j in range(i):\n",
    "                v = pi[i,j]\n",
    "                scores[i,j], scores[j,i] = abs(v), abs(v)\n",
    "                \n",
    "                if abs(v) > self._threshold:\n",
    "                    g.add_edge(i,j)\n",
    "        \n",
    "        self.scores_ = scores\n",
    "        self.network_ = g\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIXTURE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureModel():\n",
    "    def __init__(self, fdr_rate=0.1):\n",
    "        self._Q = fdr_rate\n",
    "        self.scores_ = None\n",
    "        self.network_ = None\n",
    "        \n",
    "    \n",
    "    def neg_log_likelihood(self, x, *args):\n",
    "        data = list(args[0])\n",
    "        N = len(data)\n",
    "        ll = 0.0\n",
    "        eta, k = x[0], x[1]\n",
    "        \n",
    "        for i in range(N):\n",
    "            p = data[i]\n",
    "            #f0 = (1 - p*p)**(0.5*(k-3)) * gamma(k/2) / (math.pi * gamma(0.5*(k-1)))\n",
    "            f0 = abs(p) * beta.pdf(p*p, 0.5, 0.5*(k-1))\n",
    "            fA = 0.5 # Assume the alternative distribution ~ U[-1,1]        \n",
    "            tmp = (1-eta) * f0 + eta * fA\n",
    "            ll -= log(tmp)\n",
    "\n",
    "        return ll\n",
    "\n",
    "    def optimize_mixture_model(self, X):\n",
    "        eta, k = 0.03, 100.0\n",
    "        res = opt.minimize(self.neg_log_likelihood, \n",
    "                     x0=(eta, k), \n",
    "                     args=X,\n",
    "                     bounds=((1e-6, 0.5), (10, None)),\n",
    "                     options={'gtol': 1e-6, 'disp': False})\n",
    "        return res.x\n",
    "\n",
    "\n",
    "    def calculate_p_values(self, pi_arr, k):\n",
    "        n, p_vals = len(pi_arr), []\n",
    "        for i in range(n):\n",
    "            r = pi_arr[i]\n",
    "            t_val = r * math.sqrt((k-1) / (1 - r*r))\n",
    "            p_val = 2 * (1 - ss.t.cdf(abs(t_val), k) )\n",
    "            p_vals.append((r, p_val, i))\n",
    "        return p_vals\n",
    "\n",
    "    def fit(self, X):\n",
    "        N, D = X.shape\n",
    "        E = 0.5*D*(D-1)\n",
    "        g = nx.Graph()\n",
    "        g.add_nodes_from(range(D))\n",
    "        scores = np.zeros((D,D))\n",
    "        \n",
    "        cov_shrink, _ = covar.cov_shrink_ss(X)\n",
    "        pi = cov2pcor(cov_shrink)\n",
    "\n",
    "        # Indices of the lower triangle\n",
    "        il = np.tril_indices(D, k=-1)\n",
    "        pi_elems = pi[il]\n",
    "\n",
    "        # Fit the mixture model\n",
    "        params = self.optimize_mixture_model(pi_elems)\n",
    "        eta_est, k_est = params\n",
    "        N_eff = k_est + D - 1\n",
    "\n",
    "        # Calculate the p-values using t-test\n",
    "        p_vals = self.calculate_p_values(pi_elems, k_est)\n",
    "        p_vals = sorted(p_vals, key=itemgetter(1))\n",
    "\n",
    "        # False Discovery Rate multiple testing (Benjamini and Hochberg, 1995)\n",
    "        for i, v in enumerate(p_vals):\n",
    "            r, p, idx = v\n",
    "            \n",
    "            i_idx, j_idx = il[0][idx], il[1][idx]\n",
    "            scores[i_idx, j_idx], scores[j_idx, i_idx] = p, p\n",
    "            \n",
    "            if p <= ((i+1)/E) * self._Q :\n",
    "                g.add_edge(i_idx, j_idx)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        self.network_ = g\n",
    "        self.scores_ = scores\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN SIMULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['sens', 'spec', 'fpr', 'ppv', 'mcc', 'f1_score', 'accuracy', 'hd']\n",
    "method_names = ['RN', 'PC', 'Mixture', 'GL-CV', 'GL-BIC', 'Empty']\n",
    "num_tests, num_methods, num_metrics = 200, len(method_names), len(metric_names)\n",
    "\n",
    "metrics = np.zeros((num_tests, num_methods, num_metrics))\n",
    "params = np.zeros((num_tests, 3))\n",
    "for tt in range(num_tests):\n",
    "    D = np.random.randint(low=10, high=50)\n",
    "    N = np.random.randint(low=10, high=200)\n",
    "    ne = 2 + 2*np.random.rand()\n",
    "    print('Test #%i: D=%i, N=%i, ne=%.2f' % (tt, D, N, ne))\n",
    "    eta = ne / (D-1.0)\n",
    "    params[tt,:] = [N, D, eta]\n",
    "    \n",
    "    sample_data, pcor = simulate_gaussian_data(N, D, eta)\n",
    "    actual_network = generate_graph(pcor)\n",
    "    empty_network = generate_empty_graph(pcor)\n",
    "    metrics[tt, 5, :] = generate_network_metrics(actual_network, empty_network)\n",
    "    \n",
    "    rn_model = RelevanceNetworks()\n",
    "    rn_model.fit(sample_data)\n",
    "    metrics[tt, 0, :] = generate_network_metrics(actual_network, rn_model.network_)\n",
    "    \n",
    "    pc_model = PCAlgorithm()\n",
    "    pc_model.fit(sample_data)\n",
    "    metrics[tt, 1, :] = generate_network_metrics(actual_network, pc_model.network_)\n",
    "    \n",
    "    mix_model = MixtureModel()\n",
    "    mix_model.fit(sample_data)\n",
    "    metrics[tt, 2, :] = generate_network_metrics(actual_network, mix_model.network_)\n",
    "\n",
    "    gl_model = GraphicalLasso()\n",
    "    gl_model.fit(sample_data)\n",
    "    metrics[tt, 3, :] = generate_network_metrics(actual_network, gl_model.network_)\n",
    "\n",
    "    gl_model = GraphicalLasso(use_ebic=True, gamma=0.0)\n",
    "    gl_model.fit(sample_data)\n",
    "    metrics[tt, 4, :] = generate_network_metrics(actual_network, gl_model.network_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics = np.mean(metrics, axis=0)\n",
    "metric_names = ['sens', 'spec', 'fpr', 'ppv', 'mcc', 'f1_score', 'accuracy', 'hd']\n",
    "method_names = ['RN', 'PC', 'Mixture', 'GL-CV', 'GL-BIC', 'Empty']\n",
    "\n",
    "metric_inds = [0, 3, 6, 7]\n",
    "plt.figure(figsize=(12,12))\n",
    "for i, idx in enumerate(metric_inds):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.boxplot(metrics[:50, :, idx])\n",
    "    plt.title(metric_names[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC / IAMB COMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['sens', 'spec', 'fpr', 'ppv', 'mcc', 'f1_score', 'accuracy', 'hd']\n",
    "method_names = ['PC', 'IAMB', 'Empty']\n",
    "num_tests, num_methods, num_metrics = 50, len(method_names), len(metric_names)\n",
    "\n",
    "metrics = np.zeros((num_tests, num_methods, num_metrics))\n",
    "params = np.zeros((num_tests, 3))\n",
    "\n",
    "times = np.zeros((num_tests,2))\n",
    "                \n",
    "for tt in range(num_tests):\n",
    "    N = np.random.randint(low=10, high=25)\n",
    "    D = np.random.randint(low=10, high=30)\n",
    "    ne = 1 + 2*np.random.rand()\n",
    "    print('Test #%i: D=%i, N=%i, ne=%.2f' % (tt, D, N, ne))\n",
    "    eta = ne / (D-1.0)\n",
    "    params[tt,:] = [N, D, eta]\n",
    "    \n",
    "    sample_data, pcor = simulate_gaussian_data(N, D, eta)\n",
    "    actual_network = generate_graph(pcor)\n",
    "    \n",
    "    st = time.time()\n",
    "    pc_model = PCAlgorithm()\n",
    "    pc_model.fit(sample_data)\n",
    "    metrics[tt, 1, :] = generate_network_metrics(actual_network, pc_model.network_)   \n",
    "    times[tt, 1] = time.time()-st\n",
    "    \n",
    "    iamb_model = InterIAMBAlgorithm()\n",
    "    iamb_model.fit(sample_data)\n",
    "    metrics[tt, 2, :] = generate_network_metrics(actual_network, iamb_model.network_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['sens', 'spec', 'fpr', 'ppv', 'mcc', 'f1_score', 'accuracy', 'hd']\n",
    "method_names = ['PC', 'IAMB', 'Empty']\n",
    "\n",
    "metric_inds = [0, 3, 6, 7]\n",
    "plt.figure(figsize=(12,12))\n",
    "for i, idx in enumerate(metric_inds):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.boxplot(metrics[:, :, idx])\n",
    "    plt.title(metric_names[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the E Coli data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM THE MODEL STRING\n",
    "def modelstring2graph(model_string, node_dict):\n",
    "    D = len(node_dict)\n",
    "    adj = np.zeros((D,D))\n",
    "    x = model_string\n",
    "    nodes = x[1:-1].split('][')\n",
    "    for v in nodes:\n",
    "        if \"|\" not in v:\n",
    "            continue\n",
    "        else:\n",
    "            child, parent_string = v.split('|')\n",
    "            parents = parent_string.split(':')\n",
    "            child_idx = node_dict[child]\n",
    "            for p in parents:\n",
    "                parent_idx = node_dict[p]\n",
    "                adj[child_idx][parent_idx] = 1.0\n",
    "                adj[parent_idx][child_idx] = 1.0\n",
    "    return adj\n",
    "\n",
    "fn = '/mnt/c/Users/Chris/Documents/Dissertation/Code/R/ecoli_continous_sample_1.csv'\n",
    "ecoli_data = np.genfromtxt(fn, delimiter=',', skip_header=1)\n",
    "nodes = [\"aceB\",\"asnA\",\"atpD\",\"atpG\",\"b1191\",\"b1583\",\"b1963\",\"cchB\",\"cspA\",\"cspG\",\"dnaG\",\"dnaJ\",\"dnaK\",\"eutG\",\"fixC\",\"flgD\",\"folK\",\"ftsJ\",\"gltA\",\"hupB\",\"ibpB\",\"icdA\",\"lacA\",\"lacY\",\"lacZ\",\"lpdA\",\"mopB\",\"nmpC\",\"nuoM\",\"pspA\",\"pspB\",\"sucA\",\"sucD\",\"tnaA\",\"yaeM\",\"yceP\",\"ycgX\",\"yecO\",\"yedE\",\"yfaD\",\"yfiA\",\"ygbD\",\"ygcE\",\"yhdM\",\"yheI\",\"yjbO\"]\n",
    "node_dict = {}\n",
    "modelstring = \"[b1191][cspG][eutG][cspA|cspG][fixC|b1191][sucA|eutG][yecO|cspG][yedE|cspG][atpG|sucA][cchB|fixC][dnaJ|sucA][flgD|sucA][gltA|sucA][lpdA|yedE][pspB|cspG:yedE][sucD|sucA][tnaA|b1191:fixC:sucA][yceP|eutG:fixC][yfiA|cspA][ygbD|fixC][ygcE|b1191:sucA][yhdM|sucA][yjbO|fixC][asnA|ygcE][atpD|sucA:ygcE][hupB|cspA:yfiA][ibpB|eutG:yceP][pspA|cspG:pspB:yedE][yfaD|eutG:sucA:yceP][icdA|asnA:ygcE][lacA|asnA:cspG][nmpC|pspA][yheI|atpD:yedE][aceB|icdA][b1963|yheI][dnaK|yheI][folK|yheI][lacY|asnA:cspG:eutG:lacA][ycgX|fixC:yheI][dnaG|ycgX:yheI][lacZ|asnA:lacA:lacY][nuoM|lacY][b1583|lacA:lacZ:yceP][mopB|dnaK:lacZ][yaeM|cspG:lacA:lacZ][ftsJ|mopB]\"\n",
    "for i, v in enumerate(nodes):\n",
    "    node_dict[v] = i\n",
    "\n",
    "ecoli_adj = modelstring2graph(modelstring, node_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAGIC-IRRI DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\"YR.GLASS\",\"HT\",\"YR.FIELD\",\"MIL\",\"FT\",\"G418\",\"G311\",\"G1217\",\"G800\",\"G866\",\"G795\",\"G2570\",\"G260\",\"G2920\",\"G832\",\"G1896\",\"G2953\",\"G266\",\"G847\",\"G942\",\"G200\",\"G257\",\"G2208\",\"G1373\",\"G599\",\"G261\",\"G383\",\"G1853\",\"G1033\",\"G1945\",\"G1338\",\"G1276\",\"G1263\",\"G1789\",\"G2318\",\"G1294\",\"G1800\",\"YLD\",\"FUS\",\"G1750\",\"G524\",\"G775\",\"G2835\",\"G43\"]\n",
    "node_dict = {}\n",
    "modelstring=\"[G418][G1217][G800][G866][G795][G2570][G260][G2920][G832][G2953][G847][G942][G200][G599][G261][G1853][G1033][G1945][G1338][G1263][G2318][G1750][G524][G775][G311|G1853][G1896|G2953][G257|G1217][G1373|G1750][G383|G800][G1276|G599][G1294|G418][G2835|G418][G266|G1338:G1276][G2208|G257][G1800|G1217:G2953:G257:G2835][G43|G311][HT|G832:G1896:G2953:G266:G847:G942:G2835][MIL|G1217:G2208:G1945:G1338:G524][FT|G266:G1276:G1263:G2318:G1294:G1800:G775][G1789|G266][YR.GLASS|MIL:G418:G311:G1217:G800:G866:G795:G1750][FUS|HT:G832:G1896:G383:G1853:G1033][YR.FIELD|YR.GLASS:FT:G418:G200:G257:G2208:G1373:G599:G261][YLD|YR.GLASS:HT:FT:G2570:G260:G2920:G832]\"\n",
    "for i, v in enumerate(nodes):\n",
    "    node_dict[v] = i\n",
    "\n",
    "niab_adj = modelstring2graph(modelstring, node_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARTH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\"4\",\"8\",\"13\",\"20\",\"26\",\"38\",\"47\",\"61\",\"63\",\"78\",\"81\",\"86\",\"93\",\"96\",\"100\",\"101\",\"108\",\"111\",\"126\",\"135\",\"144\",\"155\",\"161\",\"181\",\"187\",\"197\",\"198\",\"209\",\"211\",\"219\",\"226\",\"234\",\"245\",\"248\",\"256\",\"269\",\"272\",\"281\",\"289\",\"296\",\"299\",\"328\",\"331\",\"342\",\"360\",\"363\",\"368\",\"377\",\"378\",\"412\",\"414\",\"422\",\"443\",\"444\",\"452\",\"454\",\"460\",\"464\",\"479\",\"480\",\"496\",\"519\",\"537\",\"539\",\"540\",\"547\",\"554\",\"558\",\"560\",\"565\",\"570\",\"573\",\"576\",\"585\",\"596\",\"598\",\"600\",\"603\",\"622\",\"623\",\"627\",\"629\",\"636\",\"640\",\"651\",\"661\",\"665\",\"666\",\"677\",\"679\",\"686\",\"699\",\"712\",\"714\",\"726\",\"736\",\"738\",\"739\",\"758\",\"767\",\"778\",\"779\",\"781\",\"783\",\"786\",\"793\",\"798\"]\n",
    "node_dict = {}\n",
    "modelstring =\"[81][100][414][422][519][738][783][86|738][144|81][187|783][198|81][209|738][211|81][245|100][248|81][296|100][328|519][368|81][377|81][412|100][454|783][540|738][547|783][558|783][570|81:422][622|81][627|422:738][629|422][636|81:738][640|783][665|81][666|81][679|783][712|81][726|414][781|783][8|209][20|781][26|781][38|570][61|81:570][63|198:738][78|726][96|558][126|558:783][135|570][155|679][161|558][181|86:783][219|570][234|570][256|558:570][269|558:783][281|627][342|558][363|558][378|570][460|570][464|570][554|570][576|570][585|570][598|570][603|781][623|570][651|570][661|627][686|198][699|570][739|558][767|81:570][779|198][108|81:603][111|61:81:570][444|63:738][452|414:603:726][480|61:738][539|585][600|699][798|779][4|539][93|539][197|539][272|108:414:452:603:726][299|444][360|539][443|600][496|111][537|111][596|539][758|539][778|539][101|443][289|272:414:452:726][560|272:627:726][565|443][573|20:443][226|573][786|289:414:726][793|81:560:629][47|414:422:452:629:793][479|47:422:629:793][736|47:422][331|47:414:422:452:479][13|331:422][714|47:331:422:452:479:629][677|479:714]\"\n",
    "for i, v in enumerate(nodes):\n",
    "    node_dict[v] = i\n",
    "\n",
    "arth_adj = modelstring2graph(modelstring, node_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN THE EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['tp', 'fp', 'tn', 'fn', 'sens', 'spec', 'fpr', 'ppv', 'mcc', 'f1_score', 'accuracy', 'hd', 'runtime']\n",
    "method_names = ['RN', 'PC', 'InterIAMB', 'Mixture', 'Adaptive GL-CV3', 'Adaptive GL-BIC', 'Empty']\n",
    "num_tests, num_methods, num_metrics = 50, len(method_names), len(metric_names)\n",
    "\n",
    "metrics = np.zeros((num_tests, num_methods, num_metrics))\n",
    "\n",
    "actual_network = generate_graph(ecoli_adj)\n",
    "empty_network = generate_empty_graph(ecoli_adj)\n",
    "\n",
    "for tt in range(num_tests):    \n",
    "    #fn = ('/mnt/c/Users/Chris/Documents/Dissertation/Code/R/FINAL_SMALL_ECOLI_sample_%i.csv' % (tt+1))\n",
    "    #ecoli_data = np.genfromtxt(fn, delimiter=',', skip_header=1)\n",
    "    #ecoli_data = ecoli_data - ecoli_data.mean(axis=0, keepdims=True)\n",
    "    #sample_data = ecoli_data\n",
    "    print(\"Now running test #%i\" % (tt+1))\n",
    "    D = 50\n",
    "    N = 500\n",
    "    ne = 3\n",
    "    eta = ne / (D-1.0)\n",
    "    num_edges = -1\n",
    "    while(num_edges != 70):\n",
    "        sample_data, pcor = simulate_gaussian_data(N, D, eta)\n",
    "        actual_network = generate_graph(pcor)\n",
    "        empty_network = generate_empty_graph(pcor)\n",
    "        num_edges = generate_network_metrics(actual_network, empty_network, time.time()-st)[-2]\n",
    "    #st = time.time()\n",
    "    #rn_model = RelevanceNetworks()\n",
    "    #rn_model.fit(sample_data)\n",
    "    #metrics[tt, 0, :] = generate_network_metrics(actual_network, rn_model.network_, time.time()-st)\n",
    "\n",
    "    st = time.time()\n",
    "    pc_model = PCAlgorithm(threshold=0.99)\n",
    "    pc_model.fit(sample_data)\n",
    "    metrics[tt, 1, :] = generate_network_metrics(actual_network, pc_model.network_, time.time()-st)\n",
    "    st = time.time()\n",
    "    iamb_model = InterIAMBAlgorithm(threshold=0.99)\n",
    "    iamb_model.fit(sample_data)\n",
    "    metrics[tt, 2, :] = generate_network_metrics(actual_network, iamb_model.network_, time.time()-st)\n",
    "\n",
    "    st = time.time()\n",
    "    mix_model = MixtureModel(fdr_rate=0.1)\n",
    "    mix_model.fit(sample_data)\n",
    "    metrics[tt, 3, :] = generate_network_metrics(actual_network, mix_model.network_, time.time()-st)\n",
    "    \n",
    "    st = time.time()\n",
    "    gl_model = GraphicalLasso(use_ebic=False, use_adaptive=True)\n",
    "    gl_model.fit(sample_data)\n",
    "    metrics[tt, 4, :] = generate_network_metrics(actual_network, gl_model.network_, time.time()-st)\n",
    "    \n",
    "    st = time.time()\n",
    "    gl_model = GraphicalLasso(use_ebic=True, use_adaptive=True, gamma=0.0)\n",
    "    gl_model.fit(sample_data)\n",
    "    metrics[tt, 5, :] = generate_network_metrics(actual_network, gl_model.network_, time.time()-st)\n",
    "\n",
    "    st = time.time()\n",
    "    metrics[tt, 6, :] = generate_network_metrics(actual_network, empty_network, time.time()-st)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCOR Calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIG LEVEL FOR VARYING N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['tp', 'fp', 'tn', 'fn', 'sens', 'spec', 'fpr', 'ppv', 'mcc', 'f1_score', 'accuracy', 'hd', 'runtime']\n",
    "method_names = ['RN', 'PC', 'InterIAMB', 'Mixture', 'Adapative GL-CV3', 'GL-BIC', 'Empty']\n",
    "num_tests, num_methods, num_metrics = 50, len(method_names), len(metric_names)\n",
    "\n",
    "sig_levels = [[1.0-1e-6, 1.0-1e-5, 1.0-1e-4, 1.0-1e-3, 0.99, 0.975, 0.95, 0.9, 0.8],\n",
    "              [1.0-1e-7, 1.0-1e-6, 1.0-1e-5, 1.0-1e-4, 1.0-1e-3, 0.99, 0.975, 0.95, 0.9],\n",
    "              [0.0001, 0.001, 0.01, 0.02, 0.05,  0.1,  0.25, 0.5, 1.0]]\n",
    "\n",
    "N_vals = [10, 25, 50, 200, 1000]    \n",
    "#roc_curves = np.zeros((num_tests, 3, len(sig_levels[0]), len(N_vals)))\n",
    "for tt in range(num_tests):    \n",
    "    print(\"Now running test #%i\" % (tt+1))\n",
    "    D = 50\n",
    "    ne = 3\n",
    "    eta = ne / (D-1.0)\n",
    "    num_edges = -1\n",
    "    while(num_edges != 70):\n",
    "        sample_data, pcor = simulate_gaussian_data(100, D, eta)\n",
    "        actual_network = generate_graph(pcor)\n",
    "        empty_network = generate_empty_graph(pcor)\n",
    "        num_edges = generate_network_metrics(actual_network, empty_network, time.time()-st)[-2]\n",
    "    \n",
    "    for i, N in enumerate(N_vals):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        sample_data, _ = simulate_gaussian_data(N, D, -1, pcor=pcor)\n",
    "#        for idx, alpha in enumerate(sig_levels[0]):\n",
    "#            print(\"\\t PC: alpha=%.7f\" % alpha)\n",
    "#            pc_model = PCAlgorithm(threshold=alpha)\n",
    "#            pc_model.fit(sample_data)\n",
    "#            roc_curves[tt, 0, idx, i] = generate_network_metrics(actual_network, pc_model.network_)[-2]\n",
    "        \n",
    "        for idx, alpha in enumerate(sig_levels[1]):\n",
    "            # print(\"\\t IAMB: alpha=%.7f\" % alpha)\n",
    "            iamb_model = InterIAMBAlgorithm(threshold=alpha)\n",
    "            iamb_model.fit(sample_data)\n",
    "            roc_curves[tt, 1, idx, i] = generate_network_metrics(actual_network, iamb_model.network_)[-2]\n",
    "        '''\n",
    "        for idx, alpha in enumerate(sig_levels[2]):\n",
    "            print(\"\\t FDT: Q=%.7f\" % alpha)\n",
    "            mix_model = MixtureModel(fdr_rate=alpha)\n",
    "            mix_model.fit(sample_data)\n",
    "            roc_curves[tt, 2, idx, i] = generate_network_metrics(actual_network, mix_model.network_)[-2]\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(1,2,1)\n",
    "#plt.title('Effect of significance size for varying sample size', fontsize=20)\n",
    "labels = ['PC', 'InterIAMB', 'Mixture']\n",
    "for idx in range(len(N_vals)):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    x = 1-np.array(sig_levels[1])\n",
    "    tmp = roc_curves[:, 1, :, idx]\n",
    "    y = np.nanmean(tmp[:, :], axis=0)\n",
    "    yerr = np.nanstd(tmp[:, :], axis=0) / ((50**0.5) /1.96)\n",
    "#    plt.plot(x, y, 'o--', label=labels[idx])\n",
    "    plt.errorbar(x, y, yerr, xerr=None, fmt='o--', label=(\"N=%i\" % N_vals[idx]))\n",
    "plt.xlabel('1 - Sig. Level', fontsize=20)\n",
    "plt.ylabel('Hamming Distance', fontsize=20)\n",
    "plt.xscale('log')\n",
    "#plt.xlim([0,1])\n",
    "plt.title('InterIAMB Algorithm', fontsize=20)\n",
    "plt.legend(fontsize=14, loc='lower left')\n",
    "plt.suptitle('Hamming Distance of simulated Gaussian inference for varying sample sizes and parameters', fontsize=20)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['tp', 'fp', 'tn', 'fn', 'sens', 'spec', 'fpr', 'ppv', 'mcc', 'f1_score', 'accuracy', 'hd', 'runtime']\n",
    "method_names = ['RN', 'PC', 'InterIAMB', 'Mixture', 'Adapative GL-CV3', 'GL-BIC', 'Empty']\n",
    "num_tests, num_methods, num_metrics = 50, len(method_names), len(metric_names)\n",
    "\n",
    "sig_levels = [[1.0-1e-6, 1.0-1e-5, 1.0-1e-4, 1.0-1e-3, 0.99, 0.975, 0.95, 0.9, 0.8],\n",
    "              [1.0-1e-7, 1.0-1e-6, 1.0-1e-5, 1.0-1e-4, 1.0-1e-3, 0.99, 0.975, 0.95, 0.9],\n",
    "              [0.0001, 0.001, 0.01, 0.02, 0.05,  0.1,  0.25, 0.5, 1.0]]\n",
    "\n",
    "roc_curves = np.zeros((num_tests, 3, len(sig_levels[0]), num_metrics))\n",
    "    \n",
    "\n",
    "actual_network = generate_graph(arth_adj)\n",
    "empty_network = generate_empty_graph(arth_adj)\n",
    "\n",
    "for tt in range(num_tests):    \n",
    "    #fn = ('/mnt/c/Users/Chris/Documents/Dissertation/Code/R/FINAL_LARGE_ARTH_sample_%i.csv' % (tt+1))\n",
    "    #ecoli_data = np.genfromtxt(fn, delimiter=',', skip_header=1)\n",
    "    #ecoli_data = ecoli_data - ecoli_data.mean(axis=0, keepdims=True)\n",
    "    #sample_data = ecoli_data\n",
    "    print(\"Now running test #%i\" % (tt+1))\n",
    "    D = 50\n",
    "    N = 500\n",
    "    ne = 3\n",
    "    eta = ne / (D-1.0)\n",
    "    num_edges = -1\n",
    "    while(num_edges != 70):\n",
    "        sample_data, pcor = simulate_gaussian_data(N, D, eta)\n",
    "        actual_network = generate_graph(pcor)\n",
    "        empty_network = generate_empty_graph(pcor)\n",
    "        num_edges = generate_network_metrics(actual_network, empty_network, time.time()-st)[-2]\n",
    "    \n",
    "    for idx, alpha in enumerate(sig_levels[0]):\n",
    "        print(\"\\t PC: alpha=%.7f\" % alpha)\n",
    "        pc_model = PCAlgorithm(threshold=alpha)\n",
    "        pc_model.fit(sample_data)\n",
    "        roc_curves[tt, 0, idx, :] = generate_network_metrics(actual_network, pc_model.network_)\n",
    "    for idx, alpha in enumerate(sig_levels[1]):\n",
    "        print(\"\\t IAMB: alpha=%.7f\" % alpha)\n",
    "        iamb_model = InterIAMBAlgorithm(threshold=alpha)\n",
    "        iamb_model.fit(sample_data)\n",
    "        roc_curves[tt, 1, idx, :] = generate_network_metrics(actual_network, iamb_model.network_)\n",
    "    for idx, alpha in enumerate(sig_levels[2]):\n",
    "        print(\"\\t FDT: Q=%.7f\" % alpha)\n",
    "        mix_model = MixtureModel(fdr_rate=alpha)\n",
    "        mix_model.fit(sample_data)\n",
    "        roc_curves[tt, 2, idx, :] = generate_network_metrics(actual_network, mix_model.network_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sig_levels[2])\n",
    "np.nanmean(roc_curves[:, 2, :, 11], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Precision / Recall', fontsize=20)\n",
    "labels = ['PC', 'InterIAMB', 'Mixture']\n",
    "for idx in range(3):\n",
    "    x = np.nanmean(roc_curves[:, idx, :, 4], axis=0)\n",
    "    y = np.nanmean(roc_curves[:, idx, :, 7], axis=0)\n",
    "    yerr = np.nanstd(roc_curves[:, idx, :, 7], axis=0) / ((50**0.5) /1.96)\n",
    "#    x, y = np.zeros((11, 1)), np.zeros((11, 1))\n",
    "#    x[-1, 0], y[0, 0] = 1.0, 1.0\n",
    "#    x[1:-1, 0] = np.mean(roc_curves[:, idx, :, 4], axis=0)\n",
    "#    y[1:-1, 0] = np.mean(roc_curves[:, idx, :, 7], axis=0)\n",
    "#    plt.plot(x, y, 'o--', label=labels[idx])\n",
    "    plt.errorbar(x, y, yerr, xerr=None, fmt='o--', label=labels[idx])\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.legend(fontsize=14, loc='lower left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Hamming Distance / Graph Size', fontsize=20)\n",
    "labels = ['PC', 'InterIAMB', 'Mixture']\n",
    "for idx in range(3):\n",
    "    #x, y = np.zeros((18, 1)), np.zeros((18, 1))\n",
    "    #x[17, 0], y[0, 0] = 1.0, 1.0\n",
    "#    x = np.mean(roc_curves[:, idx, :, 0], axis=0) + np.mean(roc_curves[:, idx, :, 1], axis=0)\n",
    "#    y = np.mean(roc_curves[:, idx, :, 11], axis=0)\n",
    "#    plt.plot(x, y, 'o--', label=labels[idx])\n",
    "    x = np.mean(roc_curves[:, idx, :, 0], axis=0) + np.mean(roc_curves[:, idx, :, 1], axis=0)\n",
    "    y = np.mean(roc_curves[:, idx, :, 11], axis=0)\n",
    "    yerr = np.std(roc_curves[:, idx, :, 11], axis=0) / ((50**0.5)/1.96)\n",
    "#    x, y = np.zeros((11, 1)), np.zeros((11, 1))\n",
    "#    x[-1, 0], y[0, 0] = 1.0, 1.0\n",
    "#    x[1:-1, 0] = np.mean(roc_curves[:, idx, :, 4], axis=0)\n",
    "#    y[1:-1, 0] = np.mean(roc_curves[:, idx, :, 7], axis=0)\n",
    "#    plt.plot(x, y, 'o--', label=labels[idx])\n",
    "    plt.errorbar(x, y, yerr, xerr=None, fmt='o--', label=labels[idx])\n",
    "plt.xlabel('Number of edges in predicted network', fontsize=20)\n",
    "plt.ylabel('Hamming Distnace', fontsize=20)\n",
    "plt.xlim([40, 100])\n",
    "plt.ylim([0, 40])\n",
    "plt.legend(fontsize=14, loc='lower right')\n",
    "plt.suptitle('Simulated undirected Gaussian model inference, D=50, N=500', fontsize=20)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('ROC Curve for small sample Gaussian network inference', fontsize=16)\n",
    "labels = ['PC', 'InterIAMB', 'Mixture']\n",
    "for idx in range(3):\n",
    "    x, y = np.zeros((11, 1)), np.zeros((11, 1))\n",
    "    x[-1, -1], y[-1, -1] = 1.0, 1.0\n",
    "    x[1:-1, 0] = np.mean(roc_curves[:, idx, :, 4], axis=0)\n",
    "    y[1:-1, 0] = 1-np.mean(roc_curves[:, idx, :, 5], axis=0)\n",
    "    plt.plot(y, x, 'o--', label=labels[idx])\n",
    "plt.plot([0,1],[0,1], '-.')\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.xlim([0,1.0])\n",
    "plt.ylim([0.0,1])\n",
    "plt.legend(fontsize=14, loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Impact on PC for ECOLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = 50\n",
    "pc_sig_levels = [0.9999999, 0.999999, 0.99999, 0.9999, 0.999, 0.995, 0.99, 0.975, 0.95, 0.9, 0.8, 0.5]\n",
    "pc_alpha_metrics = np.zeros((num_tests, len(pc_sig_levels), 2))\n",
    "actual_network = generate_graph(ecoli_adj)\n",
    "empty_network = generate_empty_graph(ecoli_adj)\n",
    "\n",
    "for tt in range(num_tests):    \n",
    "    fn = ('/mnt/c/Users/Chris/Documents/Dissertation/Code/R/SMALL_ecoli_continous_sample_%i.csv' % (tt+1))\n",
    "    ecoli_data = np.genfromtxt(fn, delimiter=',', skip_header=1)\n",
    "    ecoli_data = ecoli_data - ecoli_data.mean(axis=0, keepdims=True)\n",
    "    sample_data = ecoli_data\n",
    "    print(\"Now running test #%i\" % (tt+1))\n",
    "    for i, alpha in enumerate(pc_sig_levels):\n",
    "        print(\"\\t PC: alpha=%.6f\" % alpha)\n",
    "        pc_model = PCAlgorithm(threshold=alpha)\n",
    "        pc_model.fit(sample_data)\n",
    "        stats = generate_network_metrics(actual_network, pc_model.network_)\n",
    "        mcc, hd = stats[8], stats[11]\n",
    "        pc_alpha_metrics[tt, i, :] = [mcc, hd]\n",
    "\n",
    "\n",
    "iamb_sig_levels = [0.9999999, 0.999999, 0.99999, 0.9999, 0.999, 0.995, 0.99, 0.975, 0.95, 0.9]\n",
    "iamb_alpha_metrics = np.zeros((num_tests, len(iamb_sig_levels), 2))\n",
    "for tt in range(num_tests):    \n",
    "    fn = ('/mnt/c/Users/Chris/Documents/Dissertation/Code/R/SMALL_ecoli_continous_sample_%i.csv' % (tt+1))\n",
    "    ecoli_data = np.genfromtxt(fn, delimiter=',', skip_header=1)\n",
    "    ecoli_data = ecoli_data - ecoli_data.mean(axis=0, keepdims=True)\n",
    "    sample_data = ecoli_data\n",
    "    print(\"Now running test #%i\" % (tt+1))\n",
    "    for i, alpha in enumerate(iamb_sig_levels):\n",
    "        mcc, hd = stats[8], stats[11]\n",
    "        \n",
    "        print(\"\\t IAMB: alpha=%.6f\" % alpha)\n",
    "        iamb_model = InterIAMBAlgorithm(threshold=alpha)\n",
    "        iamb_model.fit(sample_data)\n",
    "        stats = generate_network_metrics(actual_network, iamb_model.network_)\n",
    "        mcc, hd = stats[8], stats[11]\n",
    "        iamb_alpha_metrics[tt, i, :] = [mcc, hd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "titles = ['MCC', 'Hamming Distance']\n",
    "alphas = list(map(lambda x: 1-x, iamb_sig_levels))\n",
    "plt.figure(figsize=(12,6))\n",
    "for i, idx in enumerate(titles):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    alphas = list(map(lambda x: 1-x, iamb_sig_levels))\n",
    "    plt.plot(alphas, np.mean(iamb_alpha_metrics[:, :, i], axis=0), 'o--', label='InterIAMB')\n",
    "    alphas = list(map(lambda x: 1-x, pc_sig_levels))\n",
    "    plt.plot(alphas, np.mean(pc_alpha_metrics[:, :, i], axis=0), 'o--', label='PC')\n",
    "    plt.legend()\n",
    "    plt.ylabel(titles[i], fontsize=16)\n",
    "    plt.title(titles[i], fontsize=16)\n",
    "    plt.tick_params(labelsize=12)\n",
    "    if i == 0:\n",
    "        plt.ylim([0.5,0.65])\n",
    "    plt.xlabel('1-Sig Level', fontsize=16)\n",
    "    plt.xscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
